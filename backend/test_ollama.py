#!/usr/bin/env python3
"""
Script para probar la conectividad y respuesta de Ollama
"""
import asyncio
import httpx
import time

async def test_ollama_connection():
    """Probar conexi√≥n b√°sica con Ollama"""
    try:
        print("üîç Probando conexi√≥n con Ollama...")
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            # 1. Probar que Ollama est√© corriendo
            response = await client.get("http://localhost:11434/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                print(f"‚úÖ Ollama est√° corriendo con {len(models)} modelos:")
                for model in models[:3]:  # Solo mostrar primeros 3
                    print(f"   - {model['name']}")
                
                # 2. Probar una consulta simple con el primer modelo
                if models:
                    test_model = models[0]["name"]
                    print(f"\nüß† Probando modelo: {test_model}")
                    
                    start_time = time.time()
                    test_response = await client.post(
                        "http://localhost:11434/api/generate",
                        json={
                            "model": test_model,
                            "prompt": "Responde solo 'FUNCIONANDO' si entiendes este mensaje.",
                            "stream": False,
                            "options": {
                                "temperature": 0.1,
                                "num_predict": 50
                            }
                        }
                    )
                    end_time = time.time()
                    
                    if test_response.status_code == 200:
                        result = test_response.json()
                        ai_response = result.get("response", "")
                        print(f"‚úÖ Respuesta en {end_time - start_time:.1f}s: {ai_response[:100]}")
                        return True
                    else:
                        print(f"‚ùå Error en generaci√≥n: {test_response.status_code}")
                        return False
                else:
                    print("‚ùå No hay modelos disponibles en Ollama")
                    return False
            else:
                print(f"‚ùå Ollama no responde: {response.status_code}")
                return False
                
    except Exception as e:
        print(f"‚ùå Error conectando con Ollama: {str(e)}")
        print("   Aseg√∫rate de que Ollama est√© corriendo: ollama serve")
        return False

async def test_database_learning_simulation():
    """Simular el proceso de aprendizaje con datos m√≠nimos"""
    try:
        print("\nüìö Simulando aprendizaje de base de datos...")
        
        # Datos simulados m√≠nimos
        simple_prompt = """Aprende esta base de datos 'test_db' para generar consultas SQL.

ESQUEMA COMPACTO:
Tabla: usuarios
Columnas: id(integer)[PK], nombre(varchar)[NOT NULL], email(varchar)
Ejemplo: id='1', nombre='Juan', email='juan@test.com'

REGLAS:
1. Solo genera consultas SELECT
2. Usa nombres exactos de tablas/columnas

Responde: 'APRENDIDO - base de datos con 1 tabla usuarios'"""

        async with httpx.AsyncClient(timeout=30.0) as client:
            # Obtener primer modelo disponible
            models_response = await client.get("http://localhost:11434/api/tags")
            if models_response.status_code != 200:
                print("‚ùå No se pueden obtener modelos")
                return False
                
            models = models_response.json().get("models", [])
            if not models:
                print("‚ùå No hay modelos disponibles")
                return False
                
            test_model = models[0]["name"]
            print(f"üß† Usando modelo: {test_model}")
            
            start_time = time.time()
            response = await client.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": test_model,
                    "prompt": simple_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "top_p": 0.8,
                        "num_predict": 200,
                        "stop": ["\n\n"]
                    }
                }
            )
            end_time = time.time()
            
            if response.status_code == 200:
                result = response.json()
                ai_response = result.get("response", "")
                print(f"‚úÖ Aprendizaje simulado completado en {end_time - start_time:.1f}s")
                print(f"   Respuesta: {ai_response[:200]}")
                return True
            else:
                print(f"‚ùå Error en aprendizaje simulado: {response.status_code}")
                return False
                
    except Exception as e:
        print(f"‚ùå Error en simulaci√≥n: {str(e)}")
        return False

async def main():
    print("üöÄ PRUEBA DE OLLAMA PARA AI DATABASE CHATBOT")
    print("=" * 50)
    
    # Prueba 1: Conexi√≥n b√°sica
    if not await test_ollama_connection():
        return
    
    # Prueba 2: Simulaci√≥n de aprendizaje
    if not await test_database_learning_simulation():
        return
        
    print("\n‚úÖ TODAS LAS PRUEBAS PASARON!")
    print("Tu setup de Ollama est√° listo para el aprendizaje de bases de datos.")

if __name__ == "__main__":
    asyncio.run(main())